{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4bde5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U wxPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de5a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb186ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import wx\n",
    "\n",
    "import os; import sys; sys.path.append(os.path.join(sys.path[0], \"..\"))\n",
    "from utils import font as ufont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e237ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "OS_NAME = None\n",
    "DATA_DIR = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5247c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "os_name = (OS_NAME or platform.system()).lower()\n",
    "os_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d307c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"{0}/{1}\".format(DATA_DIR, os_name)\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1007dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prepare_chars(alphabet, *char_lists):\n",
    "    from collections import namedtuple\n",
    "\n",
    "    Char = namedtuple(\"Char\", [\"id\", \"char\", \"alphabet\", \"subset\"])\n",
    "\n",
    "    return [\n",
    "        Char(char_id, char, alphabet, subset)\n",
    "        for (char_id, char, subset) in sum(char_lists, start=[]) if char.isprintable()\n",
    "    ]\n",
    "\n",
    "def drop_duplicates_from_chars(chars):\n",
    "    return [char for char_id, char in {c.id: c for c in chars}.items()]\n",
    "\n",
    "SYMBOLS = [(ord(c), c, \"symbols\") for c in string.digits + string.punctuation + \" \"]\n",
    "\n",
    "BASIC_LATIN = [(i, chr(i), \"basic\") for i in range(ord(u'\\u0020'), ord(u'\\u007f') + 1)]\n",
    "LATIN_SUPPLEMENT = [(i, chr(i), \"supplement\") for i in range(ord(u'\\u00a0'), ord(u'\\u00ff') + 1)]\n",
    "LATIN_EXTENDED_A = [(i, chr(i), \"extended\") for i in range(ord(u'\\u0100'), ord(u'\\u017f') + 1)]\n",
    "LATIN_EXTENDED_B = [(i, chr(i), \"extended\") for i in range(ord(u'\\u0180'), ord(u'\\u024f') + 1)]\n",
    "\n",
    "BASIC_CYRILLIC = [(i, chr(i), \"basic\") for i in range(ord(u'\\u0410'), ord(u'\\u044f') + 1)]\n",
    "CYRILLIC_SUPPLEMENTARY = [(i, chr(i), \"supplement\") for i in range(ord(u'\\u0500'), ord(u'\\u052f') + 1)]\n",
    "FULL_CYRILLIC = [(i, chr(i), \"extended\") for i in range(ord(u'\\u0400'), ord(u'\\u04ff') + 1) if i not in [1155, 1156, 1157, 1158, 1159]]\n",
    "\n",
    "LATIN = prepare_chars(\"latin\", LATIN_EXTENDED_A, LATIN_EXTENDED_B, LATIN_SUPPLEMENT, BASIC_LATIN, SYMBOLS)\n",
    "CYRILLIC = prepare_chars(\"cyrillic\", FULL_CYRILLIC, CYRILLIC_SUPPLEMENTARY, BASIC_CYRILLIC)\n",
    "CHARS = sorted(drop_duplicates_from_chars(CYRILLIC + LATIN), key=lambda t: t[0])\n",
    "print(\"Characters number: {0}\".format(len(CHARS)))\n",
    "CHARS[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13cea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCALES = {\n",
    "    \"en_US\": \"latin\",\n",
    "    \"es_ES\": \"latin\",\n",
    "    \"fr_FR\": \"latin\",\n",
    "    \"pt_PT\": \"latin\",\n",
    "    \"ru_RU\": \"cyrillic\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de270e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def width_in_px(text, font=ufont.BASIC_FONT):\n",
    "    _ = wx.App()\n",
    "\n",
    "    font_info = wx.FontInfo(font.size).FaceName(font.family)\n",
    "    if font.face.bold:\n",
    "        font_info = font_info.Bold()\n",
    "    if font.face.italic:\n",
    "        font_info = font_info.Italic()\n",
    "    wx_font = wx.Font(font_info)\n",
    "\n",
    "    screen_dc = wx.ScreenDC()\n",
    "    screen_dc.SetFont(wx_font)\n",
    "    size = screen_dc.GetTextExtent(text)\n",
    "\n",
    "    return size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77157141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_widths_df(\n",
    "        font_families=ufont.FONT_FAMILIES,\n",
    "        font_sizes=ufont.FONT_SIZES,\n",
    "        font_faces=ufont.FONT_FACES,\n",
    "        chars=CHARS\n",
    "    ):\n",
    "    from itertools import product\n",
    "\n",
    "    data = {\n",
    "        \"os_name\": [],\n",
    "        \"char_id\": [],\n",
    "        \"char\": [],\n",
    "        \"alphabet\": [],\n",
    "        \"subset\": [],\n",
    "        \"font_family\": [],\n",
    "        \"font_size\": [],\n",
    "        \"font_face\": [],\n",
    "        \"is_monospaced\": [],\n",
    "        \"width\": [],\n",
    "    }\n",
    "    for (font_family, font_size, font_face, char) in product(font_families, font_sizes, font_faces, chars):\n",
    "        font = ufont.Font(font_family, font_size, font_face)\n",
    "        data[\"os_name\"] += [os_name]\n",
    "        data[\"font_family\"] += [font_family]\n",
    "        data[\"font_size\"] += [font_size]\n",
    "        data[\"font_face\"] += [str(font_face)]\n",
    "        data[\"is_monospaced\"] += [ufont.is_monospaced(font)]\n",
    "        data[\"char_id\"] += [char.id]\n",
    "        data[\"char\"] += [char.char]\n",
    "        data[\"alphabet\"] += [char.alphabet]\n",
    "        data[\"subset\"] += [char.subset]\n",
    "        data[\"width\"] += [width_in_px(char.char, font)]\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a5e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aes_texts_s(locale):\n",
    "    if locale == \"en_US\":\n",
    "        return pd.Series([\n",
    "            \"x\",\n",
    "            \"y\",\n",
    "            \"z\",\n",
    "            \"color\",\n",
    "            \"fill\",\n",
    "            \"alpha\",\n",
    "            \"shape\",\n",
    "            \"linetype\",\n",
    "            \"size\",\n",
    "            \"stacksize\",\n",
    "            \"width\",\n",
    "            \"height\",\n",
    "            \"binwidth\",\n",
    "            \"violinwidth\",\n",
    "            \"weight\",\n",
    "            \"intercept\",\n",
    "            \"slope\",\n",
    "            \"xintercept\",\n",
    "            \"yintercept\",\n",
    "            \"lower\",\n",
    "            \"middle\",\n",
    "            \"upper\",\n",
    "            \"sample\",\n",
    "            \"xmin\",\n",
    "            \"xmax\",\n",
    "            \"ymin\",\n",
    "            \"ymax\",\n",
    "            \"xend\",\n",
    "            \"yend\",\n",
    "        ], name=\"text\")\n",
    "    else:\n",
    "        return pd.Series([], dtype=str, name=\"text\")\n",
    "\n",
    "def get_pure_texts_s(size, locale, *, max_words_count=5, size_reserve_coeff=2, random_state=42):\n",
    "    from faker import Faker\n",
    "\n",
    "    Faker.seed(random_state)\n",
    "    fake = Faker(locale=locale)\n",
    "    result_df = pd.DataFrame({\n",
    "        \"text\": [\n",
    "            fake.sentence(nb_words=(int(max_words_count * n / size) + 1), variable_nb_words=False)\n",
    "            for n in list(range(size)) * size_reserve_coeff\n",
    "        ]\n",
    "    }).drop_duplicates(subset=\"text\").sample(size, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return result_df.text.str[:-1]\n",
    "\n",
    "def get_name_texts_s(size, locale, *, random_state=42):\n",
    "    from faker import Faker\n",
    "\n",
    "    Faker.seed(random_state)\n",
    "    fake = Faker(locale=locale)\n",
    "\n",
    "    return pd.Series([fake.name() for n in range(size)], dtype=str, name=\"text\")\n",
    "\n",
    "def get_date_texts_s(size, locale, *, us_only=True, random_state=42):\n",
    "    from faker import Faker\n",
    "\n",
    "    Faker.seed(random_state)\n",
    "    fake = Faker()\n",
    "    if us_only and locale != \"en_US\":\n",
    "        size = 0\n",
    "\n",
    "    return pd.Series([fake.date() for n in range(size)], dtype=str, name=\"text\")\n",
    "\n",
    "def get_latlon_texts_s(size, locale, *, us_only=True, random_state=42):\n",
    "    from faker import Faker\n",
    "\n",
    "    Faker.seed(random_state)\n",
    "    fake = Faker()\n",
    "    if us_only and locale != \"en_US\":\n",
    "        size = 0\n",
    "\n",
    "    return pd.concat([\n",
    "        pd.Series([fake.latitude() for n in range(size)], dtype=str, name=\"text\").astype(str),\n",
    "        pd.Series([fake.longitude() for n in range(size)], dtype=str, name=\"text\").astype(str),\n",
    "    ])\n",
    "\n",
    "def get_fake_texts_s(size, locale, *, random_state=42):\n",
    "    return pd.concat([\n",
    "        get_name_texts_s(size, locale, random_state=random_state),\n",
    "        get_date_texts_s(size, locale, random_state=random_state),\n",
    "        get_latlon_texts_s(round(size / 2), locale, random_state=random_state),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "def get_small_int_texts_s(limit, *, step=1):\n",
    "    return pd.Series([str(n) for n in range(-limit, limit + 1, step)], dtype=str, name=\"text\")\n",
    "\n",
    "def get_random_int_texts_s(size, *, limit=1_000_000, step=1, random_state=42):\n",
    "    return pd.DataFrame({\"text\": list(range(-limit, limit + 1, step))}).sample(size, random_state=random_state).text.astype(str)\n",
    "\n",
    "def get_small_float_texts_s(size=201, *, limit=1):\n",
    "    import numpy as np\n",
    "\n",
    "    return pd.Series([str(x) for x in np.linspace(-limit, limit, size)], dtype=str, name=\"text\")\n",
    "\n",
    "def get_random_float_texts_s(size, *, limit=1_000, random_state=42):\n",
    "    import numpy as np\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    return pd.DataFrame({\"text\": 2 * limit * np.random.random_sample(size) - limit}).text.astype(str)\n",
    "\n",
    "def get_number_texts_s(small_size, big_size, *, random_state=42):\n",
    "    return pd.concat([\n",
    "        get_small_int_texts_s(small_size),\n",
    "        get_small_float_texts_s(small_size * 2 + 1),\n",
    "        get_random_int_texts_s(big_size, random_state=random_state),\n",
    "        get_random_float_texts_s(big_size, random_state=random_state),\n",
    "    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6296704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_df(\n",
    "        font_families=ufont.FONT_FAMILIES,\n",
    "        font_sizes=ufont.FONT_SIZES,\n",
    "        font_faces=ufont.FONT_FACES,\n",
    "        locales=LOCALES,\n",
    "        texts_bunch_size=100,\n",
    "        fake_bunch_size=10,\n",
    "        numbers_small_bunch_size=10,\n",
    "        numbers_big_bunch_size=20,\n",
    "        pure_texts_only=False,\n",
    "        aes_texts=True,\n",
    "        max_words_count=5,\n",
    "        size_reserve_coeff=2,\n",
    "        random_state=42\n",
    "    ):\n",
    "    from itertools import product\n",
    "\n",
    "    def get_local_texts_df(locale, font_family, font_size, font_face, number_random_state):\n",
    "        font = ufont.Font(font_family, font_size, font_face)\n",
    "        result_s = get_pure_texts_s(texts_bunch_size, locale, max_words_count=max_words_count, random_state=random_state)\n",
    "        if not pure_texts_only:\n",
    "            result_s = pd.concat([\n",
    "                result_s,\n",
    "                get_fake_texts_s(fake_bunch_size, locale, random_state=random_state),\n",
    "                get_number_texts_s(numbers_small_bunch_size, numbers_big_bunch_size, random_state=number_random_state),\n",
    "            ], ignore_index=True)\n",
    "            if aes_texts:\n",
    "                result_s = pd.concat([result_s, get_aes_texts_s(locale)], ignore_index=True)\n",
    "        return result_s.to_frame().assign(\n",
    "            symbols_count=result_s.str.len(),\n",
    "            width=result_s.apply(lambda s: width_in_px(s, font))\n",
    "        ).sort_values([\"symbols_count\", \"width\", \"text\"]).assign(\n",
    "            os_name=lambda r: os_name,\n",
    "            alphabet=lambda r: locales[locale],\n",
    "            locale=lambda r: locale,\n",
    "            font_family=lambda r: font_family,\n",
    "            font_size=lambda r: font_size,\n",
    "            font_face=lambda r: str(font_face),\n",
    "            is_monospaced=lambda r: int(ufont.is_monospaced(font_family))\n",
    "        )\n",
    "\n",
    "    random_states = {\n",
    "        locale: random_state + i\n",
    "        for i, locale in enumerate(locales.keys())\n",
    "    }\n",
    "    df = pd.DataFrame(columns=[\"os_name\", \"text\", \"width\", \"alphabet\", \"locale\", \"font_family\", \"font_size\", \"font_face\"])\n",
    "    for (locale, font_family, font_size, font_face) in product(locales.keys(), font_families, font_sizes, font_faces):\n",
    "        df = pd.concat([df, get_local_texts_df(locale, font_family, font_size, font_face, \\\n",
    "                                               random_states[locale])], ignore_index=True)\n",
    "\n",
    "    df.symbols_count = df.symbols_count.astype(int)\n",
    "    df.is_monospaced = df.is_monospaced.astype(bool)\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec99296",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69caafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_widths_df = get_char_widths_df()\n",
    "print(char_widths_df.shape)\n",
    "char_widths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d00ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_widths_df.to_csv(\"{0}/char_widths.csv\".format(data_dir), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_df = get_texts_df(\n",
    "    texts_bunch_size=100,\n",
    "    fake_bunch_size=10,\n",
    "    numbers_small_bunch_size=10,\n",
    "    numbers_big_bunch_size=20,\n",
    "    pure_texts_only=False,\n",
    "    aes_texts=True,\n",
    "    max_words_count=4\n",
    ")\n",
    "print(control_df.shape)\n",
    "control_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39861360",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_df.to_csv(\"{0}/control.csv\".format(data_dir), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eda8d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = get_texts_df(\n",
    "    texts_bunch_size=50,\n",
    "    fake_bunch_size=20,\n",
    "    numbers_small_bunch_size=20,\n",
    "    numbers_big_bunch_size=50,\n",
    "    pure_texts_only=False,\n",
    "    aes_texts=True,\n",
    "    max_words_count=10\n",
    ")\n",
    "print(texts_df.shape)\n",
    "texts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.to_csv(\"{0}/texts.csv\".format(data_dir), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
